\section{Штучні нейронні мережі}
\subsection{Що таке нейронні мережі}
Дослідження по штучних нейронних мережах (далі "--- нейромережі) пов'язані з тим, що спосіб обробки інформації людським мозком радикально відрізняється від методів обробки інформації, які використовуються в звичайних цифрових комп'ютерах. Мозок являє собою надзвичайно \emph{складний, нелінійний, паралельний} комп'ютер (систему обробки інформації). Він здатен організовувати свої структурні компоненті, названі \emph{нейронами (neuron)}, так, щоб вони могли виконувати конкретні задачі (такі як розпізнавання образів, обробку сигналів органів чуттів, моторні функції) в багато разів швидше, ніж це можуть зробити сучасні комп'ютери (тут і далі теоретичні дані взято з \cite{Hay06}). 

Прикладом такої задачі може бути \emph{локатор (sonar)} кажана, що являє собою систему активної ехолокації. Окрім інформації про віддаль до потрібного об'єкта цей локатор надає інформацію про відносну швидкість, розміри, азимут та висоту руху об'єкта. Для виділення цієї інформації з отримуваного сигналу крихітний мозок кажана проводить складні нейронні обчислення. Ехолокація за своїми характеристиками якості та швидкості роботи значно перевищує найскладніші сучасні прилади, створені інженерами.

Що ж дає змогу мозку кажана чи людини домогтися таких вражаючих результатів? При народженні мозок має досконалу структуру, яка дає змогу створювати власні правила на основі того, що ми називаємо ``досвідом''. Досвід накопичується з часом і особливо масштабні зміни відбуваються в перші два роки життя людини. В цей час формується кістяк загальної структури. Однак розвиток на цьому не зупиняється "--- він продовжується аж до смерті людини.

Поняття розвитку нейронів пов'язане з поняттям \emph{пластичності (plasticity)} мозку "--- здатності налаштування нервової системи відповідно до зовнішнього середовища. Саме пластичність відіграє найважливішу роль в роботі нейронів як одиниць обробки інформації мозку. Аналогічно в штучних нейронних мережах робота проводиться з штучними нейронами. Загалом, \emph{нейронна мережа (neural network)} являє собою машину, яка моделює спосіб обробки мозком конкретної задачі. Для того, щоб домогтися високої продуктивності, нейромережі використовують велику кількість взаємозв'язків між елементарними частинками обчислень "--- \emph{нейронами}. Таким чином, можна дати наступне визначення нейромережам:

\begin{quotation}
	\textit{\noindent Нейронна мережа "--- це величезний розподілений паралельний процесор, що складається з елементарних одиниць обробки інформації, які накопичують експериментальні знання і надають їх для подальшої обробки. Нейромережа схожа з мозком з двох точок зору:
	\begin{enumerate}
	\item Знання поступають в нейромережу з навколишнього середовища і використовуються в процесі навчання.
	\item Для накопичення знань використовуються зв'язки між нейронами, що називаються \textbf{синаптичними вагами}.
	\end{enumerate}
	}
\end{quotation}

Процедура, яка використовується для процесу навчання, називається \emph{алгоритмом навчання (learning algorithm)}. Ця процедура вистроює в певному порядку синаптичні ваги нейромережі для забезпечення належної структури взаємозв'язків нейронів. Зміна синаптичних ваг "--- традиційний метод налаштування нейромережі.

\subsection{Переваги нейромереж}
Цілком очевидно, що свою силу нейромережі черпають, по-перше, з розпаралелювання обробки інформації, по-друге, зі здатності самонавчатися, тобто створювати узагальнення. Під терміном \emph{узагальнення (generalization)} розуміється здатність отримувати обґрунтований результат на основі даних, які не зустрічалися в процесі навчання. Ці властивості дають змогу вирішувати складні (масштабні) задачі, які на сьогодні вважаються важкорозв'язними. Однак на практиці при автономній роботі нейромережі не можуть забезпечити готових рішень. Їх необхідно інтегровувати в складні системи. Зокрема, комплексну задачу можна розбити на послідовність відносно простих, частину з яких можна розв'язати за допомогою нейромереж. 

Використання нейромереж забезпечує наступні корисні властивості систем:
\begin{itemize}
\item\emph{Нелінійність (nonlinearity).} Штучні нейрони можуть бути лінійними і нелінійними. Нейромережі, побудовані з нелінійних нейронів, самі являються нелінійними. При цьому ця нелінійність особлива, оскільки вона розподілена по всій мережі. Нелінійність "--- дуже важлива властивість, особливо якщо сам фізичний механізм, який відповідає за формування вхідного сигналу, також є нелінійним (наприклад, людська мова).
\item\emph{Відображення вхідної інформації на вихідну (input-output mapping).} Одна з популярних парадигм навчання "--- \emph{навчання з вчителем (supervised learning)}. Під цим розуміється зміна синаптичних ваг на основі набору маркованих \emph{навчальних прикладів (training sample)}. Кожен приклад складається з вхідного сигналу і відповідного йому \emph{бажаного відклику (desired response)}. З цієї множини випадковим чином вибирається приклад, а нейромережа модифікує синаптичні ваги для мінімізації відхилення сформованого нейромережею та бажаного виходу. При цьому власне модифікуються \emph{вільні параметри} мережі. Використані приклади можуть бути застосовані знову, але вже в іншому порядку. Це навчання проводиться до того часу, поки зміни синаптичних ваг не стануть незначними.
\item\emph{Адаптивність (adaptivity)}. Нейромережі мають властивість \emph{адаптовувати} свої синаптичні ваги до змін навколишнього середовища. Зокрема, нейромережі, навчені діяти в певному середовищі, можуть бути легко перенавчені для роботи в умовах незначних коливань параметрів середовища.
\item\emph{Очевидність відповіді (evidential response)}. В контексті задачі класифікації образів можна розробити нейромережу, яка збиратиме інформацію не тільки для визначення конкретного класу, але й для збільшення \emph{достовірності (confidence)} прийнятого рішення. Надалі ця інформація може використовуватися для виключення сумнівних рішень, що підвищить продуктивність нейромережі.
\item\emph{Контекстна інформація (contextual information)}. Знання представляються в самій структурі нейромережі з допомогою її стану активації. Кожен нейрон мережі потенційно може бути підданий впливу всіх інших її нейронів.
\item\emph{Стійкість до відмов (fault tolerance)}. Нейромережі, реалізовані в електронних пристроях, потенційно стійкі до відмов. Це означає, що при несприятливих умовах їх продуктивність падає незначно. Наприклад, якщо пошкоджений який-небудь нейрон чи його зв'язки, то витягування збереженої інформації утруднюється. Проте, приймаючи до уваги розподілений характер збереження інформації в нейромережі, можна стверджувати, що лише серйозні пошкодження структури нейромережі суттєво вплинуть на її працездатність.
\item\emph{Одноманітність аналізу та проектування (uniformity of analysis and design)}. Нейромережі "--- універсальний механізм обробки інформації. Це означає, що одне і те ж проектне рішення нейронної мережі може використовуватися в багатьох предметних областях. Ця властивість проявляється кількома способами:
	\begin{itemize}
	\item Нейрони в тій чи іншій формі є стандартними складовими частинами \emph{будь-якої} нейромережі.
	\item Ця загальність дає змогу використовувати одні й ті ж теорії і алгоритми навчання в різних нейромережевих програмних рішеннях.
	\item Модульні мережі можуть бути побудовані на основі інтеграції цілих модулів.
	\end{itemize}
\item\emph{Аналогія з нейробіологією (neurobiological analogy)}. Будова нейронних мереж визначається аналогією з людським мозком, який є живим доказом того, що стійкі до відмов паралельні обчислення не тільки фізично реалізовувальні, але й є швидким і потужним інструментом розв'язування задач. Нейробіологи розглядають штучні нейромережі як засіб моделювання фізичних явищ. З іншої сторони, інженери постійно намагаються почерпнути в нейробіологів нові ідеї, які виходять за рамки традиційних електросхем.
\end{itemize}

\subsection{Модель нейрона}

\emph{Нейрон} "--- одиниця обробки інформації в нейромережі. На блок-схемі (рис.~\ref{neuron_model}) показана модель нейрона, яка лежить в основі штучних нейронних мереж. В цій моделі можна виділити три основні елементи:
\begin{figure}[t] \begin{center}
\includegraphics[width=0.8\textwidth]{neuron_model.eps}
\caption{Схематична модель нейрона} \label{neuron_model}
\end{center} \end{figure}
\begin{enumerate}
\item Набір \emph{синапсів (synapse)} або \emph{зв'язків (connecting link)}, кожен з яких характеризується своєю \emph{вагою (weight)} або \emph{силою (strength)}. Зокрема, сигнал $x_j$ на вході синапса $j$, зв'язаного з нейроном $k$, множиться на вагу $w_{kj}$. Важливо звернути увагу на те, в якому порядку вказані індекси синаптичної ваги $w_{kj}$. На відміну від синапсів мозку синаптична вага штучного нейрона може мати як додатнє, так і від'ємне значення.
\item \emph{Суматор (adder)} сумує вхідні сигнали, зважені відносно відповідних синапсів нейрона. Цю операцію можна описати як лінійну комбінацію.
\item \emph{Функція активації (activation function)} обмежує амплітуду вихідного сигналу нейрона. Ця функція називається також \emph{функцією стискання (squashing function)}. Зазвичай нормалізований діапазон амплітуд виходу нейрона лежить в інтервалі $[0,1]$ або $[-1,1]$.
\end{enumerate}

В модель нейрона, показану на рис.~\ref{neuron_model}, входить також \emph{пороговий елемент (bias)}, який позначений символом $b_k$. Ця величина вказує на збільшення чи зменшення вхідного сигналу, що подається на функцію активації.

В математичному поданні функціонування нейрона $k$ можна описати наступною парою рівнянь:
\begin{equation}
u_k = \sum_{j=1}^m w_{kj} x_j,\\
\end{equation}
\begin{equation}
y_k = \varphi(u_k + b_k)
\end{equation}
де $x_1, x_2, \ldots , x_m$ "--- вхідні сигнали; $w_{k1}, w_{k2}, \ldots , w_{km}$ "--- синаптичні ваги нейрона $k$; $u_k$ "--- \emph{лінійна комбінація вхідних впливів (linear combiner output)}; $b_k$ "--- поріг; $\varphi (\cdotp)$ "--- \emph{функція активації (activation function)}; $y_k$ "--- вихідний сигнал нейрона. Використання порога $b_k$ забезпечує ефект \emph{афінного перетворення (affine transformation)} виходу лінійного суматор $u_k$. В моделі, показаній на рис.~\ref{neuron_model}, постсинаптичний потенціал обчислюється наступним чином:
\begin{equation}
v_k = u_k + b_k.
\end{equation}
\subsection{Типи функцій активації}
Функції активації, подані в формулах як $\varphi(v)$, визначають вихідний сигнал нейрона в залежності від індукованого локального поля $v$. Можна виділити три основні типи функцій активації.
\begin{figure} 
\includegraphics[width=1.0\textwidth]{function_types.eps} 
\caption{Основні типи функцій активації: функція одиничного стрибка (а); кусково-лінійна функція (б); сигмоїдальна функція для різних значень параметра $а$ (в)} \label{function_types} 
\end{figure}
\begin{enumerate}
\item\emph{Фукнція одиничного стрибка} або порогова функція (threshold function). Цей тип функції показаний на рис.~\ref{function_types},~а і описується наступним чином:
\begin{equation}
\varphi(v) = \left\{ \begin{array}{l} 1, \qquad\textrm{якщо } v \geq 0;\\ 0, \qquad\textrm{якщо } v < 0; \end{array} \right.
\end{equation}
В технічній літературі ця форма функції одиничного стрибка зазвичай називається \emph{функцією Хевісайда (Heaviside function)}. А модель нейрона, що базується на цій функції "--- \emph{моделлю Мак-Каллока-Пітца (McCalloch-Pitts)}.

\item\emph{Кусково-лінійна функція (piecewise-linear function}. Кусково-лінійна функція, показана на рис.~\ref{function_types},~б, описується наступним виразом:
\begin{equation}
\varphi(v) = \left\{ 
\begin{array}{lc} 
1, 	& v \geq +\frac{1}{2};\\
\frac{1}{2} + v, 	& -\frac{1}{2} < v < +\frac{1}{2};\\
0,		& v \leq -\frac{1}{2},\\
\end{array} \right.
\end{equation}
де коефіцієнт підсилення в лінійній області оператора припускається рівним одиниці. Цю функцію активації можна розглядати як \emph{апроксимацію (approximation)} нелінійного підсилювача. Наступні два варіанти можна вважати особливою формою кусково-лінійної функції.
\begin{itemize}
\item Якщо лінійна область оператора не досягає порогу насичення, він перетворюється в \emph{лінійний суматор (linear combiner)}.
\item Якщо коефіцієнт підсилення лінійної області прийняти нескінченно великим, то кусково-лінійна функція вироджується в порогову.
\end{itemize}
\item\emph{Сигмоїдальна функція (sigmoid function)}. Сигмоїдальна функція, графік якої нагадує літеру S є, мабуть, найбільш поширеною функцією, що використовується для створення штучних нейромереж. Це швидкозростаюча функція, яка підтримує баланс між лінійною та нелінійною поведінкою. Прикладом сигмоїдальної функції може бути \emph{логістична функція (logistic function)}, яка задається наступним виразом:
\begin{equation}
\varphi(v) = \frac{1}{1+exp(-av)},
\end{equation}
де $a$ "--- \emph{параметр нахилу (slope parameter)} сигмоїдальної функції. Змінюючи цей параметр, можна побудувати функції з різною крутизною (див.~рис.\ref{function_types}, в). Якщо порогова функція може приймати тільки значення 0 і 1, то сигмоїдальна функція може приймати будь-яке значення з діапазону $[0,1]$. При цьому варто відмітити, що сигмоїдальна функція "--- диференційовна, в той час як порогова "--- ні. (Диференційовність активаційної функції відіграє дуже важливу роль в теорії нейронних мереж, зокрема, вона є необхідною для навчання алгоритмом зворотного поширення помилки.)
\end{enumerate}

\subsection{Багатошаровий персептрон і його навчання}
Зазвичай мережа складається з сукупності сенсорних елементів (вхідних вузлів та вузлів джерела), які утворюють \emph{вхідний прошарок (input layer)}; одного або кількох \emph{прихованих прошарків (hidden layer)} обчислювальних нейронів і одного \emph{вихідного прошарку (output layer)} нейронів. Вхідний сигнал розповсюджується по мережі в прямому напрямку, від прошарку до прошарку. Такі мережі зазвичай називають \emph{багатошаровими персептронами (multilayer perceptron)}.

Багатошарові персептрони мають три характерні ознаки.

\begin{enumerate}
\item Кожен нейрон мережі має \emph{нелінійну функцію активації (nonlinear activation function)}. Важливо відмітити, що дана нелінійна функція є гладкою (тобто всюди диференційовною), на відміну від жорсткої порогової функції. Найбільш популярною формою функції, що задовільняє цю умову, є логістична функція:
\begin{displaymath}
y_j = \frac{1}{1+exp(-v_j)},
\end{displaymath}
де $v_j$ "--- індуковане локальне поле (тобто зважена сума всіх синаптичних входів плюс порогове значення) нейрона $j$; $y_j$ "--- вихід нейрона. Наявність нелінійності відіграє дуже важливу роль, так як в іншому випадку відображення ``вхід-вихід'' мережі можна звести до звичайного одношарового персептрону. Більше того, використання логістичної функції мотивовано біологічно, оскільки в ній враховується відновлювальна фаза реального нейрона.
\item Мережа містить один або декілька прошарків \emph{прихованих нейронів}, які не є частиною входу або виходу мережі. Ці нейрони дають змогу мережі навчатися вирішенню складних задач, послідовно виокремлюючи найбільш важливі ознаки з вхідного образа (вектора).
\item Мережа має великий рівень \emph{зв'язності (connectivity)}, яка реалізується через синаптичні зв'язки. Зміна рівня зв'язності мережі потребує зміни сукупності синаптичних зв'язків або їх вагових коефіцієнтів.
\end{enumerate}

Комбінація всіх цих властивостей разом зі здатністю до навчання на власному досвіді забезпечує обчислювальну потужність багатошарового персептрона. Однак ці ж якості є також причною неповноти сучасних знань про поведінку мереж такого типу. По-перше, розподілена форма нелінійності і високий рівень зв'язності мережі суттєво ускладнюють теоретичний аналіз багатошарового персептрону. По-друге, наявність прихованих нейронів робить процес навчання більш складним для візуалізації. Саме в процесі навчання необхідно визначити, які ознаки вхідного сигналу треба представляти прихованим нейронам. Тоді процес навчання стає ще більш складним, оскільки пошук повинен виконуватися в дуже широкій області можливих функцій, а вибір повинен проводитися серед альтернативних подань вхідних образів.

На рис.~\ref{multilayer_perceptron} показаний архітектурний граф багатошарового персептрона з двома прихованими прошарками і одним вихідним прошарком.
\begin{figure} 
\includegraphics[width=1.0\textwidth]{multilayer_perceptron.eps} 
\caption{Архітектурний граф багатошарового персептрона} \label{multilayer_perceptron} 
\end{figure}

\subsection{Алгоритм зворотного поширення помилки}

Багатошарові персептрони успішно застосовуються для розв'язання різноманітних складних задач. При цьому навчання з вчителем відбувається з допомогою такого популярного алгоритму, як \emph{алгоритм зворотного поширення помилки (error back-propagation algorithm)}. Цей алгоритм базується на \emph{корекції помилок (error-correction learning rule)}.

Навчання методом зворотного поширення помилки вимагає два проходи по всіх прошарках мережі: прямого і зворотного. При \emph{прямому проході (forward pass)} образ (вхідний вектор) подається на сенсорні вузли мережі, після чого поширюється по мережі від прошарку до прошарку. В результаті генерується набір вихідних сигналів, який і є фактичною реакцією мережі на даний вхідний образ. Підчас прямого проходу всі синаптичні ваги фіксовані. Підчас \emph{зворотного проходу (backward pass)} всі синаптичні ваги налаштовуються згідно правила корекції помилок, а саме: фактичний вихід мережі віднімається від бажаного (цільового) відклику, в результаті чого формується сигнал помилки \emph{(error signal)}. Цей сигнал згодом розповсюджується по мережі в напрямку, зворотному напрямку синаптичних зв'язків. Звідси й назва "--- алгоритм зворотного поширення помилки. Синаптичні ваги налаштовуються з метою максимального наближення вихідного сигналу мережі до бажаного в статистичному сенсі. Процес навчання, який реалізується даним алгоритмом, називається \emph{навчанням на основі зворотного поширення (back-propagation learning)}.

Алгоритм зворотного поширення помилки циклічно обробляє приклади з навчальної множини $\left\{ \left( x(n), d(n) \right) \right\}_{n=1}^N$, де $N$ "--- кількість навчальних прикладів, наступним чином:

\begin{enumerate}
\item\emph{Ініціалізація (initialization)}. Припускаючи відсутність апріорної інформації, генеруємо синаптичні ваги і порогові значення з допомогою генератора рівномірно розподілених випадкових чисел з середнім значенням 0. Дисперсія вибирається таким чином, щоб стандартне відхилення індукованого локального поля нейронів припадало на лінійну частину сигмоїдальної функції активації (і не досягало області насичення).
\item\emph{Надання прикладів навчання (presentation of training examples)}. В мережу подаються образи з навчальної множини (епохи). Для кожного образу послідовно виконується прямий та зворотній проходи, описані далі в пп.~3~та~4. 
\item\emph{Прямий прохід (forward computation)}. Нехай навчальний приклад поданий парою $\left(\mathbf{x}(n),\mathbf{d}(n)\right)$, де $\mathbf{x}(n)$ "--- вхідний вектор, який надається вхідному прошарку сенсорних вузлів; $\mathbf{d}(n)$ "--- бажаний відклик, який надається вихідному прошарку нейронів для формування сигналу помилки. Обчислюємо індуковані локальні поля і функціональні сигнали мережі, проходячи по ній по прошарках в прямому напрямі. Індуковане локальне поле нейрона $j$ прошарку $l$ обчислюється за формулою
\begin{equation}
v_j^{(l)} = \sum_{i=0}^{m_{l-1}} w_{ji}^{(l)}(n) y_i^{(l-1)}(n), 
\end{equation}
де $y_i^{(l-1)}(n)$ "--- вихідний (функціональний) сигнал нейрона $i$, розміщеного в попередньому прошарку $l-1$, на ітерації $n$; $w_{ji}^{(l)}(n)$ "--- синаптична вага зв'язку нейрона $j$ прошарку $l$ з нейроном $i$ прошарку $l-1$. Для $i = 0\quad y_0^{(l)}(n)=+1$, а $w_{j0}^{(l)}(n)=b_j^l(n)$ "--- поріг, який застосовується до нейрона $j$ прошарку $l$. Якщо використовується сигмоїдальна функція, то вихідний сигнал нейрона $j$ прошарку $l$ виражається наступним чином:
$$
y_j^{(l)}(n) = \varphi_j(v_j^l(n)).
$$
Якщо нейрон $j$ знаходиться в першому прихованому прошарку (тобто $l=1$), то
$$
y_j^{(0)}(n) = x_j(n),
$$
де $x_j(n)$ "--- $j$-й елемент вхідного вектора $\mathbf{x}(n)$. Якщо нейрон $j$ знаходиться в вихідному прошарку (тобто $l=L$, де $L$ "--- глибина мережі), то
\begin{equation}
e_j(n) = d_j(n) - o_j(n),
\end{equation}
де $d_j(n)$ "--- $j$-й елемент вектора бажаного відклику $\mathbf{d}(n)$.
\item\emph{Зворотній прохід (backward computation)}. Обчислюємо локальні градієнти вузлів мережі за наступною формулою:
\begin{equation}
\delta _j^{(l)}(n) = \left[ 
\begin{array}{l}
e_j^{(L)}(n)\varphi_j'(v_j^{(L)}(n)),\\
		\hspace{1.8cm}\textrm{для нейрона } j \textrm{ вихідного прошарку } L,\\
\varphi_j'(v_j^{(l)}(n))\sum_{k=0}^{m_{l+1}} \delta_k^{(l+1)}(n) w_{kj}^{(l+1)}(n), \\
		\hspace{1.8cm}\textrm{для нейрона } j \textrm{ прихованого прошарку } l,\\
\end{array}
\right.
\end{equation}
де штрих в функції $\varphi_j'(\cdotp)$ позначає диференціювання по аргументу. Зміна синаптичних ваг прошарку $l$ мережі виконується згідно з узагальненим дельта-правилом
\begin{equation}
w_{ji}^{(l)}(n+1) = w_{ji}^{(l)}(n) + \alpha[w_{ji}^{(l)}(n-1)] + \eta\delta_j^{(l)}(n)y_j^{(l-1)}(n),
\end{equation}
де $\eta$ "--- параметр швидкості навчання (крок навчання); $\alpha$ "--- стала моменту.
\item\emph{Ітерації (iteration)}. Послідовно виконуємо прямий та зворотній проходи (згідно пп. 3, 4), надаючи мережі всі навчальні приклади з епохи, поки не буде досягнуто критерію зупинки.
\end{enumerate}

\emph{Зауваження}. Порядок надання навчальних прикладів може випадковим чином змінюватися від епохи до епохи. Параметри моменту та швидкості навчання налаштовуються (і зазвичай зменшуються) з ростом кількості ітерацій. 
